#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2017/8/15 15:30
# @Author  : ELP
# @Site    :
# @File    : Task1_S-TFIDF.py
# @Software: PyCharm

import jieba.analyse
import re
import csv
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
import time

'''
To run this python file, please:
1.install jieba 0.38; load python 2.7
2.add path of text data: in line 54
-----------------------------------------------
Time(test data): it takes about 3000s~3300s to run off this S-TFIDF model on Mac OS Sierra 10.12.6
                 (2.4 GHz Intel Core i5; 8 GB 1600 MHz DDR3)
'''

start = time.clock()

#Load self-defined mian dictionary
jieba.load_userdict(r"data/dictionary.txt")

#Load training data and extract all answers unrepeatablely
file_answer='data/train/SMPCUP2017_TrainingData_Task1.txt'
f_answer=list(open(file_answer, "r").readlines())
blog_id_answer = [s.strip().split('\001')[0] for s in f_answer]
key_words1=[s.strip().split('\001')[1] for s in f_answer]
key_words2=[s.strip().split('\001')[2] for s in f_answer]
key_words3=[s.strip().split('\001')[3] for s in f_answer]
key_words4=[s.strip().split('\001')[4] for s in f_answer]
key_words5=[s.strip().split('\001')[5] for s in f_answer]
keywords=[]
keywords.extend(key_words1)
keywords.extend(key_words2)
keywords.extend(key_words3)
keywords.extend(key_words4)
keywords.extend(key_words5)
keywords1=set(keywords)

#load test data
file_answer_test='data/test/SMPCUP2017_TestSet_Task1.txt'
f_answer_test=list(open(file_answer_test, "r").readlines())
blog_id_test = [s.strip().split('\001')[0] for s in f_answer_test]

#load text data
contentfile='add path of your text data'

#load self-defined stopwords dictionary
cis = []
for ci in open("data/stopwords.txt", "r"):
    cis.append(ci.strip())

#load self-defined idf dictionary
dic={}
for line in open(r'data/idf_dictionary.txt','r').readlines():
    dic[line.split(' ')[0]]=line.split(' ')[1]

#load self-defined 42topic-top100-words dictionary,which is generated by LDA
dicc={}
for line in open("data/42topicTo100words.txt", "r"):
    L=list(line.strip().split(" "))
    for l in L:
        if dicc.get(l)==None:
            dicc[l]=1
        else:
            dicc[l]=dicc[l]+1

#store results in a CSV file
csvfile=file("res/task1_S-TFIDF.csv","w")
writer=csv.writer(csvfile)
writer.writerow(['contentid','keyword1','keyword2',',keyword3'])

#weight adjustment for TopN keywords selected by Jieba
def words_process(words):
    for word in words:
        #weight adjustment for Chinese words and phrases if they are in self-defined idf dictionary
        if dic.get(word) != None and re.match(u"[\u4e00-\u9fa5]+", word):
            words[word] = float(jieba.lcut(eachline).count(word)) * float(dic[word])
        #weight adjustment for phrases and words containg english
        if re.match("[a-z]+", word):
            words[word] *= 1.7
    for key in words:
        #weigth adjustment for words and phrases if they are in self-defined 42topic-top100-words dictionary
        if dicc.get(key) != None:
            '''
            the formula "(1.0 / float(dicc[key]) - 1.0 / 42.0)" was drawn lessons from the fifth winner of CCF contest:
            https://github.com/coderSkyChen/2016CCF_BDCI_Sougou
            '''
            words[key] = words[key] * (1.0 / float(dicc[key]) - 1.0 / 42.0)
        else:
            words[key] = words[key]
    return sorted(words.iteritems(), key=lambda d: d[1], reverse=True)

for eachline in open(contentfile,'r'):

    blog_id = eachline.split('\001')[0]

    if blog_id in blog_id_test:
        blog_title = eachline.split('\001')[1].decode("utf-8").lower()
        blog_content = eachline.split('\001')[2].decode("utf-8").lower()

        #add extra weight for the title of the document
        material=(blog_title + " ") * 12 + blog_content

        #extract all english words entirely from thedocument and temporarily add them into main dictionary
        L=set(re.findall('[a-z0-9]+',blog_title+" "+blog_content))
        n = set()
        for m in L:
            if m.isdigit()!=True:
                jieba.add_word(m)
                jieba.suggest_freq(m,True)
                n.add(m)

        s=[]
        words = {}
        for x,weight in jieba.analyse.extract_tags(material, topK=20, withWeight=True, allowPOS=('n')):
            if (x not in cis) and (x.isdigit()==False):
                #weight adjustment for keywords according to their length
                words[x]=weight*float(len(x))
        i=0
        for word in words_process(words):
            s.append(word[0])
            if i>=14:
                break
            i+=1

        r = []
        words1 = {}
        for x1,weight1 in jieba.analyse.textrank(material, topK=20, withWeight=True, allowPOS=('n')):
            if (x1 not in cis) and (x1.isdigit()==False):
                words1[x1]=weight1*float(len(x1))
        j=0
        for word1 in words_process(words1):
            r.append(word1[0])
            if j>=14:
                break
            j+=1

        #preserve words or phrases that are both extracted by textrank and tfidf and used to selected in train keywords
        u=[]
        u.append(blog_id)
        for key in s:
            if key in r and key in keywords:
                u.append(key)

        if len(u)<5:
            #if the keywords of the document are less than 3, get new words orderly in the selection of promoted tfidf model
            words3 = {}
            for x3, weight3 in jieba.analyse.extract_tags(material, withWeight=True, topK=5, allowPOS=('n')):
                if (x3 not in cis) and (x3.isdigit() == False) and (x3 not in u):
                    words3[x3] = weight3 * float(len(x3))
            for word3 in words_process(words3):
                u.append(word3[0])
                if len(u)>=5:
                    break

        #return top3 keywords
        writer.writerow(u[:4])

        #delete all temporarily added words and phrases in line 115-121
        for key in n:
            jieba.del_word(key)

csvfile.close()

end = time.clock()
print '\n'
print "read: %f s" % (end - start)